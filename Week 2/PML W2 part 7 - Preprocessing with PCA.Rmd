---
title: "Preprocessing with PCA"
output:
    html_document:
        keep_md: true
---

```{r setup, cache=FALSE, echo=FALSE, message=F, warning=F, tidy=FALSE}
require(knitr)
options(width=100)
opts_chunk$set(message=F, error=F, warning=F, comment=NA, fig.align='center', dpi=100, tidy=F, cache.path='.cache/', fig.path='fig/')

options(xtable.type='html')
knit_hooks$set(inline=function(x) {
    if(is.numeric(x)) {
        round(x, getOptions('digits'))
    } else {
        paste(as.character(x), collapse=', ')
    }
})
knit_hooks$set(plot=knitr:::hook_plot_html)
```

## Correlated Predictors

```{r}
set.seed(32323)
library(caret)
library(kernlab)
data(spam)
inTrain <- createDataPartition(y=spam$type, p=0.75, list=F)
training <- spam[inTrain,]
testing <- spam[-inTrain,]

M <- abs(cor(training[,-58]))
diag(M) <- 0
which(M > 0.8, arr.ind=T)
```

---

## Correlated Predictors

```{r}
names(spam)[c(34,32)]
plot(spam[,34],spam[,32])
```

---

## Basic PCA Idea

- We might not need every predictor
- A weighted combination of predictors might be better
- We should pick this combination to capture the "most information" possible
- Benefits
    - Reduced number of predictors
    - Reduced noise (due to averaging)

---

## We Could Rotate the Plot

$$
X = 0.71 \times \mbox{num}415 + 0.71 \times \mbox{num}857 \\
Y = 0.71 \times \mbox{num}415 - 0.71 \times \mbox{num}857
$$

```{r}
X <- 0.71 * training$num415 + 0.71 * training$num857
Y <- 0.71 * training$num415 - 0.71 * training$num857
plot(X, Y)
```

---

## Related Problems

You have multivariate variables $X_1,...X_n$ so $X_1 = (X_{11},...,X{1m})$

- Find a new set of multivariate variables that are uncorrelated and explain as much variance as possible
- If you put all the variables together in one matrix, find the best matrix created with fewer variables (lower rank) that explains the original data

The first goal is **statistical** and the second goal is **data compression**

---

## Related Solutions - PCA/SVD

**SVD**

If $X$ is a matrix with each variable in a column and each observation in a row then the SVD is a "matrix decomposition"

$$
X = UDV^T
$$

where the columns of $U$ are orthogonal (left singular vectors), the columns of $V$ ae orthogonal (right singular vectors) and $D$ is a diagonal matrix (singular values)

**PCA**

The principal components are equal to the right singular values if you first scale (subtract the mean, divide by the standard deviation) the variables

---

## Principal Components in R - `prcomp`

```{r}
smallSpam <- spam[,c(34,32)]
prComp <- prcomp(smallSpam)
plot(prComp$x[,1], prComp$x[,2])
```

---

## Principal Components in R - `prcomp`

```{r}
prComp$rotation
```

---

## PCA on SPAM Data

```{r}
typeColor <- ((spam$type=="spam")*1 + 1)
prComp <- prcomp(log10(spam[,-58]+1))
plot(prComp$x[,1], prComp$x[,2], col=typeColor, xlab="PC1", ylab="PC2")
```

---

## PCA with Caret

```{r}
preProc <- preProcess(log10(spam[,-58]+1), method="pca", pcaComp=2)
spamPC <- predict(preProc, log10(spam[,-58]+1))
plot(spamPC[,1], spamPC[,2], col=typeColor)
```

---

## Preprocessing with PCA

```{r}
preProc <- preProcess(log10(training[,-58] + 1), method="pca", pcaComp=2)
trainPC <- predict(preProc, log10(training[,-58] + 1))
modelFit <- train(training$type ~ ., method="glm", data=trainPC)
```

---

## Preprocessing with PCA

```{r}
testPC <- predict(preProc, log10(testing[,-58] + 1))
confusionMatrix(testing$type, predict(modelFit, testPC))
```

---

## Alternative (defaults the # of PCs)

```{r}
modelFit <- train(training$type ~ ., method="glm", preProcess="pca", data=training)
confusionMatrix(testing$type, predict(modelFit, testing))
```

---

## Final thoughts on PCs

- Most useful for linear-type models
- Can make it harder to interpret predictors
- Watch out for outliers!
    - Transform first (with logs or Box Cox)
    - Plot predictors to identify problems
- For more info see
    - Exploratory Data Analysis
    - [Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/)