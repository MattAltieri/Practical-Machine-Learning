---
title: "Boosting"
output:
    html_document:
        keep_md: true
---

```{r setup, cache=FALSE, echo=FALSE, message=F, warning=F, tidy=FALSE}
require(knitr)
options(width=100)
opts_chunk$set(message=F, error=F, warning=F, comment=NA, fig.align='center', dpi=100, tidy=F, cache.path='.cache/', fig.path='fig/')

options(xtable.type='html')
knit_hooks$set(inline=function(x) {
    if(is.numeric(x)) {
        round(x, getOptions('digits'))
    } else {
        paste(as.character(x), collapse=', ')
    }
})
knit_hooks$set(plot=knitr:::hook_plot_html)
```

## Basic Idea

1. Take lots of (possibly) weak predictors
2. Weight them and add them up
3. Get a stronger predictor

---

## Basic Idea Behind Boosting

1. Start with a set of classifiers $h_1,...,h_k$
    - Examples: All possible trees, all possible regression models, all possible cutoffs
2. Create a classifier that combines classification functions: $f(x) = \mbox{sgn}\left(\sum_{t=1}^T \alpha_t h_t(x)\right)$
    - Goal is to minimize error (on training set)
    - Iterative, select one $h$ at each step
    - Calculate weights based on errors
    - Upweight missed classifications and select next $h$

[Adaboost on Wikipedia](https://en.wikipedia.org/wiki/AdaBoost)

[http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)

---

## Simple Example

![](exboosting.JPG)

[http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)

---

## Round 1: adaboost

![](adaboost1.JPG)

[http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)

---

## Round 2 & 3

![](adaboost2-3.JPG)

[http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)

---

## Completed Classifier

![](adaboost4.JPG)

[http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)

---

## Boosting in R

- Boosting can be used with any subset of classifiers
- One large subclass is [gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting)
- R has multiple boosting libraries. Differences include the choice of basic classification functions and combination rules
    - [gbm](https://cran.r-project.org/web/packages/gbm/index.html) -- boosting with trees
    - [mboost](https://cran.r-project.org/web/packages/mboost/index.html) -- model-based boosting
    - [ada](https://cran.r-project.org/web/packages/ada/index.html) -- statistical boosting based on [additive logistic regression](http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1016218223)
    - [gamBoost](https://cran.r-project.org/web/packages/GAMBoost/index.html) -- for boosting generalized additive models
- Most of these are available in the `caret` package

---

## Wage Example -- Fit the Model

```{r}
library(caret)
library(ISLR)
data(Wage)
wage <- subset(Wage, select=-(logwage))
train.flags <- createDataPartition(y=wage$wage, p=.7, list=F)
wage.train <- wage[train.flags,]
wage.test <- wage[-train.flags,]
fit <- train(wage ~ ., wage.train, method="gbm", verbose=F)
print(fit)
```

---

## Plot the Results

```{r}
qplot(predict(fit, wage.test), wage, data=wage.test)
```

---

## Notes and Further Reading

- A couple of nice tutorials for boosting
    - Freund and Shapire -- [http://www.cc.gatech.edu/~thad/6601-gradAI-fall2013/boosting.pdf](http://www.cc.gatech.edu/~thad/6601-gradAI-fall2013/boosting.pdf)
    - Ron Meir -- [http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf
- Boosting, random forests, and model ensembling are the most common tools that win Kaggle and other prediction contests
    - [http://www.netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf](http://www.netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf)
    - [https://kaggle2.blob.core.windows.net/wiki-files/327/09ccf652-8c1c-4a3d-b979-ce2369c985e4/Willem%20Mestrom%20-%20Milestone%201%20Description%20V2%202.pdf](https://kaggle2.blob.core.windows.net/wiki-files/327/09ccf652-8c1c-4a3d-b979-ce2369c985e4/Willem%20Mestrom%20-%20Milestone%201%20Description%20V2%202.pdf)